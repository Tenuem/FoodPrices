{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc8d31b",
   "metadata": {},
   "source": [
    "The objective of this project is to go through ETL process using PySpark and prepare a simple Data Warehouse for food item prices tracking in XXI century Poland. The dataset is free to download from FAOSTAT webpage: https://www.fao.org/faostat/en/#data/PP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FoodPrices\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89523894",
   "metadata": {},
   "source": [
    "After creating a spark session, let's have a look at the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cfaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/Prices_E_Europe_NOFLAG.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30a085",
   "metadata": {},
   "source": [
    "As a result of an ETL process we want to get a specific information about food prices in Poland in specific months during XXI century.\n",
    "Let's start filtering our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feceb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.filter(col('Area') == 'Poland')\n",
    "df = df.filter(col('Months') != 'Annual value')\n",
    "\n",
    "df = df.filter(col('Unit') == 'LCU') # tylko rekordy z cenami w lokalnej walucie -> PLN\n",
    "\n",
    "year_columns = [c for c in df.columns \n",
    "                 if c.startswith(\"Y20\") or c.startswith(\"Y21\")] # don't take years before 2000\n",
    "\n",
    "df = df.select(*[\"Item Code\", \"Item\", \"Months\"], *year_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193c40f",
   "metadata": {},
   "source": [
    "We are leaving only necessary columns for further analysis (for example leaving area columns since all would have the same value).\n",
    "Let's see the result top 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48421c0",
   "metadata": {},
   "source": [
    "Having an output dataframe, let's limit number of columns by pivoting - creating different row for every year with corresponding month. This way we change an orientation of the table from wider to longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4952ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_expression = \"stack({0}, {1}) as (Year, Price)\".format(\n",
    "    len(year_columns),\n",
    "    \", \".join([f\"'{y[1:]}', `{y}`\" for y in year_columns])  # Delete leading YXXXX in the 'year'\n",
    ")\n",
    "\n",
    "df = df.selectExpr(\"*\", pivot_expression).drop(*year_columns) \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6aa2e",
   "metadata": {},
   "source": [
    "We can see that after the transformation we have some rows where 'Price'=None. Since this is a crucial value for our analysis, we can get rid of the records that do not have it - there is no sense keeping the records without measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"Price\").isNotNull())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1263a5f",
   "metadata": {},
   "source": [
    "Now, having prepared our data in this way we can proceed to model it for a simple data warehouse.\n",
    "Our fact table will consist of price value, while dimensions will be time(date) and item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba806f0d",
   "metadata": {},
   "source": [
    "Let's start from dimension tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_item = df.select(\"Item Code\", \"Item\").distinct() \\\n",
    "    .withColumnRenamed(\"Item Code\", \"id\") \\\n",
    "    .withColumnRenamed(\"Item\", \"name\")\n",
    "\n",
    "dim_item.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e385903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is more convinient to have a number corresponding to the month apart from its name\n",
    "months_dict = {\n",
    "    \"January\": 1, \"February\": 2, \"March\": 3, \"April\": 4,\n",
    "    \"May\": 5, \"June\": 6, \"July\": 7, \"August\": 8,\n",
    "    \"September\": 9, \"October\": 10, \"November\": 11, \"December\": 12\n",
    "}\n",
    "\n",
    "df_months = spark.createDataFrame(months_dict.items(), [\"month_name\", \"month_num\"])\n",
    "\n",
    "dim_date = df.select(\"Year\", \"Months\").distinct() \\\n",
    "    .join(df_months, df.Months == df_months.month_name, \"left\") \\\n",
    "    .withColumnRenamed(\"Year\", \"year\") \\\n",
    "    .withColumnRenamed(\"month_num\", \"month_num\") \\\n",
    "    .drop(\"Months\") # avoid redundation \n",
    "\n",
    "dim_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f69089",
   "metadata": {},
   "source": [
    "Let's also add an information about a corresponding quarter to each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dim_date = dim_date.withColumn(\n",
    "    \"quarter\",\n",
    "    F.when(F.col(\"month_num\").between(1, 3), \"Q1\")\n",
    "     .when(F.col(\"month_num\").between(4, 6), \"Q2\")\n",
    "     .when(F.col(\"month_num\").between(7, 9), \"Q3\")\n",
    "     .otherwise(\"Q4\")\n",
    ")\n",
    "\n",
    "dim_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb212c",
   "metadata": {},
   "source": [
    "Every table, for the sake of speed, should have an id that is unique and consist of only one column. \n",
    "In the item dimension it is already satisfied since every food item has its unique id.\n",
    "For the date dimension table we will create one artificial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dim_date = dim_date.withColumn(\"id\", monotonically_increasing_id())\n",
    "dim_date.filter(col('year') == 2023).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65847d19",
   "metadata": {},
   "source": [
    "Now we can create our fact table.\n",
    "It is crucial to connect it to our dimension tables based on the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34077b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df.select(\n",
    "    col(\"Item Code\").alias(\"item_id\"),\n",
    "    col(\"year\"),\n",
    "    col(\"Months\"),\n",
    "    col(\"Price\").alias(\"price_pln\")\n",
    ").alias(\"f\")\n",
    "\n",
    "d = dim_date.alias(\"d\")\n",
    "i = dim_item.alias(\"i\")\n",
    "\n",
    "fact_prices = f.join(\n",
    "        d,\n",
    "        (col(\"f.year\") == d.year) &\n",
    "        (col(\"f.Months\") == d.month_name),\n",
    "        \"left\"\n",
    "    ).join(\n",
    "        i,\n",
    "        (col(\"f.item_id\") == i.id)\n",
    "    ).select(\n",
    "        col(\"f.item_id\"), \n",
    "        col(\"d.id\").alias(\"date_id\"),\n",
    "        col(\"price_pln\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab932b",
   "metadata": {},
   "source": [
    "Now we can look at out DW tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FACT TABLE: food prices\")\n",
    "fact_prices.show()\n",
    "\n",
    "print(\"DIMENSION TABLE: food items\")\n",
    "dim_item.show()\n",
    "\n",
    "print(\"DIMENSION TABLE: dates\")\n",
    "dim_date.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d0418",
   "metadata": {},
   "source": [
    "For this DW we can start writing some analytical queries to get insights on the food market.\n",
    "\n",
    "Let's start from extracting the most expensive product in the last year (on the average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ids of the dates with 2023 year (last year with available data)\n",
    "dates2023 = dim_date.select('id').filter(col('year') == 2023)\n",
    "\n",
    "avg_prices = (\n",
    "    fact_prices.join(dates2023, col(\"date_id\") == dates2023.id, \"inner\")\n",
    "            .groupBy(\"item_id\")\n",
    "            .agg(F.avg(\"price_pln\").alias(\"avg_price\"))\n",
    ")\n",
    "\n",
    "# sort and take the highest one only\n",
    "most_expensive_product = avg_prices.orderBy(F.desc(\"avg_price\")).limit(1)\n",
    "\n",
    "# get item name from dimension table\n",
    "most_expensive_product = (\n",
    "    most_expensive_product\n",
    "        .join(i, col(\"item_id\") == i.id, \"left\")\n",
    "        .select(col(\"i.name\"), col(\"avg_price\"))\n",
    ")\n",
    "\n",
    "most_expensive_product.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271d8c2",
   "metadata": {},
   "source": [
    "Now let's try to get the fastest growing prices products in the last year. For this, let's take an average price of Q4 in corresponding years (2022 & 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average price for each quarter\n",
    "quarters = (fact_prices\n",
    "                .join(d, fact_prices.date_id == d.id, \"inner\")\n",
    "                .filter(col(\"d.quarter\") == \"Q4\")\n",
    "                .groupBy(\"f.item_id\", \"d.year\")\n",
    "                .agg(F.avg(\"price_pln\").alias(\"avg_price\"))\n",
    ")\n",
    "\n",
    "q4_2023 = quarters.filter(col(\"year\") == 2023).select(\n",
    "    \"item_id\", col(\"avg_price\").alias(\"avg_2023\"))\n",
    "\n",
    "q4_2022 = quarters.filter(col(\"year\") == 2022).select(\n",
    "    \"item_id\", col(\"avg_price\").alias(\"avg_2022\"))\n",
    "\n",
    "# calculate price difference (both absolute and percentage)\n",
    "price_change = (q4_2022.join(q4_2023, \"item_id\", \"inner\")\n",
    "           .withColumn(\"abs_change\", col(\"avg_2023\") - col(\"avg_2022\"))\n",
    "           .withColumn(\"%_change\", (col(\"abs_change\") / col(\"avg_2022\")) * 100)\n",
    ")\n",
    "\n",
    "# add item name instead of id\n",
    "price_change = (price_change\n",
    "                .join(i, i.id == price_change.item_id, 'left')\n",
    "                .drop('item_id', 'price_change.id')\n",
    ")\n",
    "\n",
    "price_change = price_change.orderBy(col(\"%_change\").desc())\n",
    "price_change.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
