{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc8d31b",
   "metadata": {},
   "source": [
    "The objective of this project is to go through ETL process using PySpark and prepare a simple Data Warehouse for food item prices tracking in XXI century Poland. The dataset is free to download from FAOSTAT webpage: https://www.fao.org/faostat/en/#data/PP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f79ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FoodPrices\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89523894",
   "metadata": {},
   "source": [
    "After creating a spark session, let's have a look at the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6cfaf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/c:/proggrind/FoodPrices/data/data/Prices_E_Europe_NOFLAG.csv. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/Prices_E_Europe_NOFLAG.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\proggrind\\FoodPrices\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:838\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\proggrind\\FoodPrices\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\proggrind\\FoodPrices\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/c:/proggrind/FoodPrices/data/data/Prices_E_Europe_NOFLAG.csv. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/Prices_E_Europe_NOFLAG.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30a085",
   "metadata": {},
   "source": [
    "As a result of an ETL process we want to get a specific information about food prices in Poland in specific months during XXI century.\n",
    "Let's start filtering our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feceb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.filter(col('Area') == 'Poland')\n",
    "df = df.filter(col('Months') != 'Annual value')\n",
    "\n",
    "df = df.filter(col('Unit') == 'LCU') # tylko rekordy z cenami w lokalnej walucie -> PLN\n",
    "\n",
    "year_columns = [c for c in df.columns \n",
    "                 if c.startswith(\"Y20\") or c.startswith(\"Y21\")] # don't take years before 2000\n",
    "\n",
    "df = df.select(*[\"Item Code\", \"Item\", \"Months\"], *year_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193c40f",
   "metadata": {},
   "source": [
    "We are leaving only necessary columns for further analysis (for example leaving area columns since all would have the same value).\n",
    "Let's see the result top 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f3ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Item Code=44, Item='Barley', Months='January', Y2000=None, Y2001=None, Y2002=None, Y2003=None, Y2004=None, Y2005=None, Y2006=None, Y2007=None, Y2008=None, Y2009=None, Y2010=429.0, Y2011=778.0, Y2012=789.0, Y2013=885.0, Y2014=772.0, Y2015=632.0, Y2016=633.0, Y2017=600.0, Y2018=676.0, Y2019=818.0, Y2020=711.0, Y2021=717.0, Y2022=1104.0, Y2023=1258.0, Y2024=None),\n",
       " Row(Item Code=44, Item='Barley', Months='February', Y2000=None, Y2001=None, Y2002=None, Y2003=None, Y2004=None, Y2005=None, Y2006=None, Y2007=None, Y2008=None, Y2009=None, Y2010=418.0, Y2011=756.0, Y2012=806.0, Y2013=862.0, Y2014=734.0, Y2015=619.0, Y2016=611.0, Y2017=603.0, Y2018=673.0, Y2019=832.0, Y2020=676.0, Y2021=759.0, Y2022=1116.0, Y2023=1154.0, Y2024=None),\n",
       " Row(Item Code=44, Item='Barley', Months='March', Y2000=None, Y2001=None, Y2002=None, Y2003=None, Y2004=None, Y2005=None, Y2006=None, Y2007=None, Y2008=None, Y2009=None, Y2010=415.0, Y2011=806.0, Y2012=816.0, Y2013=841.0, Y2014=735.0, Y2015=606.0, Y2016=598.0, Y2017=620.0, Y2018=671.0, Y2019=773.0, Y2020=670.0, Y2021=804.0, Y2022=1228.0, Y2023=1058.0, Y2024=None),\n",
       " Row(Item Code=44, Item='Barley', Months='April', Y2000=None, Y2001=None, Y2002=None, Y2003=None, Y2004=None, Y2005=None, Y2006=None, Y2007=None, Y2008=None, Y2009=None, Y2010=398.0, Y2011=785.0, Y2012=858.0, Y2013=810.0, Y2014=740.0, Y2015=595.0, Y2016=590.0, Y2017=622.0, Y2018=674.0, Y2019=780.0, Y2020=678.0, Y2021=834.0, Y2022=1390.0, Y2023=940.0, Y2024=None),\n",
       " Row(Item Code=44, Item='Barley', Months='May', Y2000=None, Y2001=None, Y2002=None, Y2003=None, Y2004=None, Y2005=None, Y2006=None, Y2007=None, Y2008=None, Y2009=None, Y2010=403.0, Y2011=783.0, Y2012=838.0, Y2013=804.0, Y2014=718.0, Y2015=566.0, Y2016=594.0, Y2017=638.0, Y2018=662.0, Y2019=788.0, Y2020=693.0, Y2021=842.0, Y2022=1463.0, Y2023=868.0, Y2024=None)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48421c0",
   "metadata": {},
   "source": [
    "Having an output dataframe, let's limit number of columns by pivoting - creating different row for every year with corresponding month. This way we change an orientation of the table from wider to longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4952ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Item Code=44, Item='Barley', Months='January', Year='2000', Price=None),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2001', Price=None),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2002', Price=None),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2003', Price=None),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2004', Price=None)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_expression = \"stack({0}, {1}) as (Year, Price)\".format(\n",
    "    len(year_columns),\n",
    "    \", \".join([f\"'{y[1:]}', `{y}`\" for y in year_columns])  # Delete leading YXXXX in the 'year'\n",
    ")\n",
    "\n",
    "df = df.selectExpr(\"*\", pivot_expression).drop(*year_columns) \n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6aa2e",
   "metadata": {},
   "source": [
    "We can see that after the transformation we have some rows where 'Price'=None. Since this is a crucial value for our analysis, we can get rid of the records that do not have it - there is no sense keeping the records without measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d253f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Item Code=44, Item='Barley', Months='January', Year='2010', Price=429.0),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2011', Price=778.0),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2012', Price=789.0),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2013', Price=885.0),\n",
       " Row(Item Code=44, Item='Barley', Months='January', Year='2014', Price=772.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(col(\"Price\").isNotNull())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1263a5f",
   "metadata": {},
   "source": [
    "Now, having prepared our data in this way we can proceed to model it for a simple data warehouse.\n",
    "Our fact table will consist of price value, while dimensions will be time(date) and item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba806f0d",
   "metadata": {},
   "source": [
    "Let's start from dimension tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e7f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=15, name='Wheat'),\n",
       " Row(id=71, name='Rye'),\n",
       " Row(id=882, name='Raw milk of cattle'),\n",
       " Row(id=1145, name='Meat of rabbits and hares, fresh or chilled (biological)'),\n",
       " Row(id=187, name='Peas, dry')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_item = df.select(\"Item Code\", \"Item\").distinct() \\\n",
    "    .withColumnRenamed(\"Item Code\", \"id\") \\\n",
    "    .withColumnRenamed(\"Item\", \"name\")\n",
    "\n",
    "dim_item.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e385903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='2011', month_name='August', month_num=8),\n",
       " Row(year='2014', month_name='December', month_num=12),\n",
       " Row(year='2023', month_name='September', month_num=9),\n",
       " Row(year='2011', month_name='February', month_num=2),\n",
       " Row(year='2010', month_name='May', month_num=5)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is more convinient to have a number corresponding to the month apart from its name\n",
    "months_dict = {\n",
    "    \"January\": 1, \"February\": 2, \"March\": 3, \"April\": 4,\n",
    "    \"May\": 5, \"June\": 6, \"July\": 7, \"August\": 8,\n",
    "    \"September\": 9, \"October\": 10, \"November\": 11, \"December\": 12\n",
    "}\n",
    "\n",
    "df_months = spark.createDataFrame(months_dict.items(), [\"month_name\", \"month_num\"])\n",
    "\n",
    "dim_date = df.select(\"Year\", \"Months\").distinct() \\\n",
    "    .join(df_months, df.Months == df_months.month_name, \"left\") \\\n",
    "    .withColumnRenamed(\"Year\", \"year\") \\\n",
    "    .withColumnRenamed(\"month_num\", \"month_num\") \\\n",
    "    .drop(\"Months\") # avoid redundation \n",
    "\n",
    "dim_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f69089",
   "metadata": {},
   "source": [
    "Let's also add an information about a corresponding quarter to each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb465f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='2011', month_name='August', month_num=8, quarter='Q3', UUID='1d7e66f0-cab1-47fc-944a-b368069c435c', id=0),\n",
       " Row(year='2014', month_name='December', month_num=12, quarter='Q4', UUID='79983a0f-d1a3-4f8c-b51d-aedf3ddae799', id=1),\n",
       " Row(year='2023', month_name='September', month_num=9, quarter='Q3', UUID='bbc0753c-4fd2-41fb-bd4d-99e5b96e78c0', id=2),\n",
       " Row(year='2011', month_name='February', month_num=2, quarter='Q1', UUID='aeed3d10-2aaf-408c-9317-e7d83567de6d', id=3),\n",
       " Row(year='2010', month_name='May', month_num=5, quarter='Q2', UUID='0d89eed5-d11a-4880-a49f-767d14957d34', id=4)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dim_date = dim_date.withColumn(\n",
    "    \"quarter\",\n",
    "    F.when(F.col(\"month_num\").between(1, 3), \"Q1\")\n",
    "     .when(F.col(\"month_num\").between(4, 6), \"Q2\")\n",
    "     .when(F.col(\"month_num\").between(7, 9), \"Q3\")\n",
    "     .otherwise(\"Q4\")\n",
    ")\n",
    "\n",
    "dim_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb212c",
   "metadata": {},
   "source": [
    "Every table, for the sake of speed, should have an id that is unique and consist of only one column. \n",
    "In the item dimension it is already satisfied since every food item has its unique id.\n",
    "For the date dimension table we will create one artificial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='2011', month_name='August', month_num=8, quarter='Q3', id=0),\n",
       " Row(year='2014', month_name='December', month_num=12, quarter='Q4', id=1),\n",
       " Row(year='2023', month_name='September', month_num=9, quarter='Q3', id=2),\n",
       " Row(year='2011', month_name='February', month_num=2, quarter='Q1', id=3),\n",
       " Row(year='2010', month_name='May', month_num=5, quarter='Q2', id=4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "dim_date = dim_date.withColumn(\"id\", monotonically_increasing_id())\n",
    "dim_date.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65847d19",
   "metadata": {},
   "source": [
    "Now we can create our fact table.\n",
    "It is crucial to connect it to our dimension tables based on the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34077b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|item_id|date_id|price_pln|\n",
      "+-------+-------+---------+\n",
      "|     44|     98|    756.0|\n",
      "|     44|    140|    603.0|\n",
      "|   1121|     98|   6843.0|\n",
      "|   1121|    140|   7510.0|\n",
      "|     56|     98|    975.0|\n",
      "|     56|    140|    510.0|\n",
      "|    945|     98|   5875.0|\n",
      "|    945|    140|   5750.0|\n",
      "|   1095|     98|   3859.0|\n",
      "|   1095|    140|   3460.0|\n",
      "|   1056|     98|   4758.0|\n",
      "|   1056|    140|   4160.0|\n",
      "|   1145|     98|   5784.0|\n",
      "|   1145|    140|   8110.0|\n",
      "|   1013|     98|   7164.0|\n",
      "|   1013|    140|   8800.0|\n",
      "|     75|     98|    603.0|\n",
      "|     75|    140|    454.0|\n",
      "|    116|     98|    393.0|\n",
      "|    116|    140|    357.0|\n",
      "+-------+-------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "f = df.select(\n",
    "    col(\"Item Code\").alias(\"item_id\"),\n",
    "    col(\"year\"),\n",
    "    col(\"Months\"),\n",
    "    col(\"Price\").alias(\"price_pln\")\n",
    ").alias(\"f\")\n",
    "\n",
    "d = dim_date.alias(\"d\")\n",
    "i = dim_item.alias(\"i\")\n",
    "\n",
    "fact_prices = f.join(\n",
    "        d,\n",
    "        (col(\"f.year\") == d.year) &\n",
    "        (col(\"f.Months\") == d.month_name),\n",
    "        \"left\"\n",
    "    ).join(\n",
    "        i,\n",
    "        (col(\"f.item_id\") == i.id)\n",
    "    ).select(\n",
    "        col(\"f.item_id\"), \n",
    "        col(\"d.id\").alias(\"date_id\"),\n",
    "        col(\"price_pln\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab932b",
   "metadata": {},
   "source": [
    "Now we can look at out DW tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf2cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACT TABLE: food prices\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fact_prices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFACT TABLE: food prices\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mfact_prices\u001b[49m.show()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDIMENSION TABLE: food items\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m dim_item.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'fact_prices' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"FACT TABLE: food prices\")\n",
    "fact_prices.show()\n",
    "\n",
    "print(\"DIMENSION TABLE: food items\")\n",
    "dim_item.show()\n",
    "\n",
    "print(\"DIMENSION TABLE: dates\")\n",
    "dim_date.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a179bd",
   "metadata": {},
   "source": [
    "For this DW we can start writing some analytical queries to get insights on the food market.\n",
    "\n",
    "Let's start from extracting the most expensive product in the last year (on the average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ea0a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dim_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Extract ids of the dates with 2024 year\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dates2024 = \u001b[43mdim_date\u001b[49m.select(\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m).filter(col(\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m) == \u001b[32m2024\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(dates2024)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mtype\u001b[39m(dates2024)\n",
      "\u001b[31mNameError\u001b[39m: name 'dim_date' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract ids of the dates with 2024 year\n",
    "dates2024 = dim_date.select('id').filter(col('year') == 2024)\n",
    "print(dates2024)\n",
    "type(dates2024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
